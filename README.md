    μ©μ–΄ νμ•…μ„ μ„ν• κΈ°μ΄ μ •λ¦¬πΊ

### λ”¥λ¬λ‹ κ°λ…λ„

<img src="./deep_arch.PNG"  width="600" height="auto">

### μ¤‘μ” κ°λ…(μμ£Ό μ‚¬μ©ν•λ” μ©μ–΄ λ§ μ •λ¦¬)

1. Optimizer
    * Optimizerλ€, λ¨λΈμ΄ ν•™μµμ„ ν†µν•΄ μµμ μ **κ°€μ¤‘μΉ(weight)**μ™€ **νΈν–¥(bias)**μ„ μ°Ύμ„ μ μλ„λ΅ λ„μ™€μ£Όλ” μ•κ³ λ¦¬μ¦
    * λ”¥λ¬λ‹ λ¨λΈμ€ μ…λ ¥ λ°μ΄ν„°μ™€ μ΄μ— λ€ν• **Label(μ •λ‹µ) λ°μ΄ν„°**λ¥Ό μ‚¬μ©ν•μ—¬ μμΈ΅μ„ μν–‰
    * μμΈ΅ κ²°κ³Όμ™€ μ‹¤μ  κ²°κ³Ό μ‚¬μ΄μ **Loss(μ¤μ°¨)**κ°€ λ°μƒ
    * μ¤μ°¨λ¥Ό μµμ†ν™” ν•κΈ° μ„ν•΄ Optimizerλ¥Ό μ‚¬μ©
    * λ€ν‘μ μΈ Optimizerλ” **Adam** μ‚¬μ©, μµμ‹  νΈλ λ“λ” Adam W μ‚¬μ©

2. Batch
    * λ”¥λ¬λ‹μ—μ„ batchλ€, **ν•™μµ λ°μ΄ν„°**λ¥Ό **μΌμ •ν• ν¬κΈ°μ λ¬¶μ λ‹¨μ„λ΅ μ²λ¦¬**ν•λ” κ²ƒμ„ μλ―Έ
    * κ°€μ¤‘μΉμ™€ νΈν–¥μ„ μ—…λ°μ΄νΈν•  λ• λ¨λ“  λ°μ΄ν„°λ¥Ό ν•λ²μ— μ²λ¦¬ν•λ©΄ μ—°μ‚° λΉ„μ©μ΄ ν¬κΈ° λ•λ¬Έμ— μΌμ •ν• ν¬κΈ°μ λ¬¶μ(batch) λ‹¨μ„λ΅ μ²λ¦¬ν•μ—¬ ν•™μµ μ†λ„μ™€ ν¨μ¨μ„±μ„ κ°μ„ 

3. Learning Rate
    * Optimizerκ°€ λ¨λΈμ κ°€μ¤‘μΉ(weight)λ¥Ό μ—…λ°μ΄νΈν•  λ•, **μ΄μ „ κ°€μ¤‘μΉ κ°’μ—μ„ μ–Όλ§λ‚ ν¬κ² μ—…λ°μ΄νΈν• μ§€λ¥Ό κ²°μ •ν•λ” ν•μ΄νΌνλΌλ―Έν„°**

4. Epochs
    * λ°μ΄ν„°λ¥Ό λ„£λ” νμλ¥Ό 1 epochsλΌκ³  ν•¨
    * ex. λ°μ΄ν„° 15λ§κ° μ¤‘ 5λ§κ°λ¥Ό λ„£μ—λ‹¤κ³  κ°€μ •ν•λ©΄, 3 epochs

5. Cross Entropy
    * λ”¥λ¬λ‹μ—μ„ λ¶„λ¥(classification) λ¬Έμ μ—μ„ λ§μ΄ μ‚¬μ©λλ” μ†μ‹¤ ν•¨μ(loss function) μ¤‘ ν•λ‚

6. Transfer Learning
    * λ”¥λ¬λ‹μ—μ„λ” μΌλ°μ μΌλ΅ μ΄λ―Έμ§€λ‚ ν…μ¤νΈ λ°μ΄ν„° λ“±μ—μ„ λ§μ΄ μ‚¬μ©
    * μ „μ΄ ν•™μµμ„ μ‚¬μ©ν•λ©΄ μ μ€ μ–‘μ λ°μ΄ν„°λ¥Ό κ°€μ§€κ³ λ„ λ†’μ€ μ„±λ¥μ„ λ‹¬μ„±